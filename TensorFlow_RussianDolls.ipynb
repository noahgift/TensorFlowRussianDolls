{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow-RussianDolls.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1ilOvhjA3xS1",
        "WguU1NgEsCL6",
        "DskmJEsu8xdX",
        "QDDm6TsK34PC",
        "ItXfxkxvosLH",
        "NvoiEwiAWrWy",
        "Aa-KnZKcfvkV",
        "yVLSSAUViyiX",
        "W3G5sZ3yo-yK",
        "ggjuE4es7SDH"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahgift/TensorFlowRussianDolls/blob/master/TensorFlow_RussianDolls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dRxWhH0g3tKF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Russian Dolls\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EggdJbEp6Pak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![wikipedia_commons](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Russian_Dolls.jpg/640px-Russian_Dolls.jpg)\n",
        "\n",
        "[*source:  wikipedia*](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Russian_Dolls.jpg/640px-Russian_Dolls.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "g5uaujBRJ7Z9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A *crash course* involving several layers of abstraction with [TensorFlow](https://www.tensorflow.org/)"
      ]
    },
    {
      "metadata": {
        "id": "1ilOvhjA3xS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WguU1NgEsCL6"
      },
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Hello World"
      ]
    },
    {
      "metadata": {
        "id": "oDpT_ks65v4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "*  [Official Hello World Example]( https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=oYZkU7ZN3CL0)\n",
        "*  Adds two matrices\n"
      ]
    },
    {
      "metadata": {
        "id": "5iX0wEVI6t49",
        "colab_type": "code",
        "outputId": "291f3746-2cd6-4e77-95a4-c7b263ad9e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "input1 = tf.ones((2, 3))\n",
        "input2 = tf.reshape(tf.range(1, 7, dtype=tf.float32), (2, 3))\n",
        "\n",
        "print(\"Two Tensor Flow Matrices with shape:\")\n",
        "print(input1.shape)\n",
        "print(input2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Two Tensor Flow Matrices with shape:\n",
            "(2, 3)\n",
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "afyOLD6p5l3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = input1 + input2\n",
        "with tf.Session():\n",
        "  result = output.eval()\n",
        "\n",
        "print(f\"Type of result:  {type(result)}\\n\")\n",
        "print(\"Result of addition of two Matrics:\")\n",
        "result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ng1Yv1pXUhWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensorflow Linear Regression (Project)"
      ]
    },
    {
      "metadata": {
        "id": "xZdpALUHUw6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based *on official Google Tutorial [here](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises)*"
      ]
    },
    {
      "metadata": {
        "id": "Bd2Zkk1LE2Zr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "  * Learn fundamental TensorFlow concepts\n",
        "  * Use the `LinearRegressor` class in TensorFlow to predict median housing price, at the granularity of city blocks, based on one input feature\n",
        "  * Evaluate the accuracy of a model's predictions using Root Mean Squared Error (RMSE)\n",
        "  * Improve the accuracy of a model by tuning its hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "MxiIKhP4E2Zr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The [data](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) is based on 1990 census data from California."
      ]
    },
    {
      "metadata": {
        "id": "6TjLjL9IU80G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "In this first cell, we'll load the necessary libraries."
      ]
    },
    {
      "metadata": {
        "id": "rVFf5asKE2Zt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipRyUHjhU80Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll load our data set."
      ]
    },
    {
      "metadata": {
        "id": "9ivCDWnwE2Zx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVk_qlG6U80j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll randomize the data, just to be sure not to get any pathological ordering effects that might harm the performance of Stochastic Gradient Descent. Additionally, we'll scale `median_house_value` to be in units of thousands, so it can be learned a little more easily with learning rates in a range that we usually use."
      ]
    },
    {
      "metadata": {
        "id": "r0eVyguIU80m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe = california_housing_dataframe.reindex(\n",
        "    np.random.permutation(california_housing_dataframe.index))\n",
        "california_housing_dataframe[\"median_house_value\"] /= 1000.0\n",
        "california_housing_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzzlSs3PtTmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Examine the Data\n",
        "\n",
        "It's a good idea to get to know your data a little bit before you work with it.\n",
        "\n",
        "We'll print out a quick summary of a few useful statistics on each column: count of examples, mean, standard deviation, max, min, and various quantiles."
      ]
    },
    {
      "metadata": {
        "id": "gzb10yoVrydW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lr6wYl2bt2Ep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build the First Model\n",
        "\n",
        "In this exercise, we'll try to predict `median_house_value`, which will be our label (sometimes also called a target). We'll use `total_rooms` as our input feature.\n",
        "\n",
        "**NOTE:** Our data is at the city block level, so this feature represents the total number of rooms in that block.\n",
        "\n",
        "To train our model, we'll use the [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor) interface provided by the TensorFlow [Estimator](https://www.tensorflow.org/get_started/estimator) API. This API takes care of a lot of the low-level model plumbing, and exposes convenient methods for performing model training, evaluation, and inference."
      ]
    },
    {
      "metadata": {
        "id": "0cpcsieFhsNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Define Features and Configure Feature Columns"
      ]
    },
    {
      "metadata": {
        "id": "EL8-9d4ZJNR7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. There are two main types of data we'll use in this and future exercises:\n",
        "\n",
        "* **Categorical Data**: Data that is textual. In this exercise, our housing data set does not contain any categorical features, but examples you might see would be the home style, the words in a real-estate ad.\n",
        "\n",
        "* **Numerical Data**: Data that is a number (integer or float) and that you want to treat as a number. As we will discuss more later sometimes you might want to treat numerical data (e.g., a postal code) as if it were categorical.\n",
        "\n",
        "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**. Feature columns store only a description of the feature data; they do not contain the feature data itself.\n",
        "\n",
        "To start, we're going to use just one numeric input feature, `total_rooms`. The following code pulls the `total_rooms` data from our `california_housing_dataframe` and defines the feature column using `numeric_column`, which specifies its data is numeric:"
      ]
    },
    {
      "metadata": {
        "id": "rhEbFCZ86cDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the input feature: total_rooms.\n",
        "my_feature = california_housing_dataframe[[\"total_rooms\"]]\n",
        "\n",
        "# Configure a numeric feature column for total_rooms.\n",
        "feature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_3S8teX7Rd2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The shape of our `total_rooms` data is a one-dimensional array (a list of the total number of rooms for each block). This is the default shape for `numeric_column`, so we don't have to pass it as an argument."
      ]
    },
    {
      "metadata": {
        "id": "UMl3qrU5MGV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Define the Target"
      ]
    },
    {
      "metadata": {
        "id": "cw4nrfcB7kyk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll define our target, which is `median_house_value`. Again, we can pull it from our `california_housing_dataframe`:"
      ]
    },
    {
      "metadata": {
        "id": "l1NvvNkH8Kbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the label.\n",
        "targets = california_housing_dataframe[\"median_house_value\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4M-rTFHL2UkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Configure the LinearRegressor"
      ]
    },
    {
      "metadata": {
        "id": "fUfGQUNp7jdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll configure a linear regression model using LinearRegressor. We'll train this model using the `GradientDescentOptimizer`, which implements Mini-Batch Stochastic Gradient Descent (SGD). The `learning_rate` argument controls the size of the gradient step.\n",
        "\n",
        "**NOTE:** To be safe, we also apply [gradient clipping](https://developers.google.com/machine-learning/glossary/#gradient_clipping) to our optimizer via `clip_gradients_by_norm`. Gradient clipping ensures the magnitude of the gradients do not become too large during training, which can cause gradient descent to fail. "
      ]
    },
    {
      "metadata": {
        "id": "ubhtW-NGU802",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use gradient descent as the optimizer for training the model.\n",
        "my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\n",
        "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "\n",
        "# Configure the linear regression model with our feature columns and optimizer.\n",
        "# Set a learning rate of 0.0000001 for Gradient Descent.\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=feature_columns,\n",
        "    optimizer=my_optimizer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0IztwdK2f3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Define the Input Function"
      ]
    },
    {
      "metadata": {
        "id": "S5M5j6xSCHxx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To import our California housing data into our `LinearRegressor`, we need to define an input function, which instructs TensorFlow how to preprocess\n",
        "the data, as well as how to batch, shuffle, and repeat it during model training.\n",
        "\n",
        "First, we'll convert our *pandas* feature data into a dict of NumPy arrays. We can then use the TensorFlow [Dataset API](https://www.tensorflow.org/programmers_guide/datasets) to construct a dataset object from our data, and then break\n",
        "our data into batches of `batch_size`, to be repeated for the specified number of epochs (num_epochs). \n",
        "\n",
        "**NOTE:** When the default value of `num_epochs=None` is passed to `repeat()`, the input data will be repeated indefinitely.\n",
        "\n",
        "Next, if `shuffle` is set to `True`, we'll shuffle the data so that it's passed to the model randomly during training. The `buffer_size` argument specifies\n",
        "the size of the dataset from which `shuffle` will randomly sample.\n",
        "\n",
        "Finally, our input function constructs an iterator for the dataset and returns the next batch of data to the LinearRegressor."
      ]
    },
    {
      "metadata": {
        "id": "RKZ9zNcHJtwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a linear regression model of one feature.\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "  \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwa6UeA1V5F_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE:** We'll continue to use this same input function in later exercises. For more\n",
        "detailed documentation of input functions and the `Dataset` API, see the [TensorFlow Programmer's Guide](https://www.tensorflow.org/programmers_guide/datasets)."
      ]
    },
    {
      "metadata": {
        "id": "4YS50CQb2ooO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "yP92XkzhU803",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now call `train()` on our `linear_regressor` to train the model. We'll wrap `my_input_fn` in a `lambda`\n",
        "so we can pass in `my_feature` and `target` as arguments (see this [TensorFlow input function tutorial](https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model) for more details), and to start, we'll\n",
        "train for 100 steps."
      ]
    },
    {
      "metadata": {
        "id": "5M-Kt6w8U803",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = linear_regressor.train(\n",
        "    input_fn = lambda:my_input_fn(my_feature, targets),\n",
        "    steps=100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Nwxqxlx2sOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Evaluate the Model"
      ]
    },
    {
      "metadata": {
        "id": "KoDaF2dlJQG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's make predictions on that training data, to see how well our model fit it during training.\n",
        "\n",
        "**NOTE:** Training error measures how well your model fits the training data, but it **_does not_** measure how well your model **_generalizes to new data_**. In later exercises, you'll explore how to split your data to evaluate your model's ability to generalize.\n"
      ]
    },
    {
      "metadata": {
        "id": "pDIxp6vcU809",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create an input function for predictions.\n",
        "# Note: Since we're making just one prediction for each example, we don't \n",
        "# need to repeat or shuffle the data here.\n",
        "prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n",
        "\n",
        "# Call predict() on the linear_regressor to make predictions.\n",
        "predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
        "\n",
        "# Format predictions as a NumPy array, so we can calculate error metrics.\n",
        "predictions = np.array([item['predictions'][0] for item in predictions])\n",
        "\n",
        "# Print Mean Squared Error and Root Mean Squared Error.\n",
        "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
        "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
        "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AKWstXXPzOVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Is this a good model? How would you judge how large this error is?\n",
        "\n",
        "Mean Squared Error (MSE) can be hard to interpret, so we often look at Root Mean Squared Error (RMSE)\n",
        "instead.  A nice property of RMSE is that it can be interpreted on the same scale as the original targets.\n",
        "\n",
        "Let's compare the RMSE to the difference of the min and max of our targets:"
      ]
    },
    {
      "metadata": {
        "id": "7UwqGbbxP53O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
        "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
        "min_max_difference = max_house_value - min_house_value\n",
        "\n",
        "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
        "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
        "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
        "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JigJr0C7Pzit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our error spans nearly half the range of the target values. Can we do better?\n",
        "\n",
        "This is the question that nags at every model developer. Let's develop some basic strategies to reduce model error.\n",
        "\n",
        "The first thing we can do is take a look at how well our predictions match our targets, in terms of overall summary statistics."
      ]
    },
    {
      "metadata": {
        "id": "941nclxbzqGH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "calibration_data = pd.DataFrame()\n",
        "calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "calibration_data[\"targets\"] = pd.Series(targets)\n",
        "calibration_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2-bf8Hq36y8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay, maybe this information is helpful. How does the mean value compare to the model's RMSE? How about the various quantiles?\n",
        "\n",
        "We can also visualize the data and the line we've learned.  Recall that linear regression on a single feature can be drawn as a line mapping input *x* to output *y*.\n",
        "\n",
        "First, we'll get a uniform random sample of the data so we can make a readable scatter plot."
      ]
    },
    {
      "metadata": {
        "id": "SGRIi3mAU81H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample = california_housing_dataframe.sample(n=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-JwuJBKU81J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll plot the line we've learned, drawing from the model's bias term and feature weight, together with the scatter plot. The line will show up red."
      ]
    },
    {
      "metadata": {
        "id": "7G12E76-339G",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the min and max total_rooms values.\n",
        "x_0 = sample[\"total_rooms\"].min()\n",
        "x_1 = sample[\"total_rooms\"].max()\n",
        "\n",
        "# Retrieve the final weight and bias generated during training.\n",
        "weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\n",
        "bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
        "\n",
        "# Get the predicted median_house_values for the min and max total_rooms values.\n",
        "y_0 = weight * x_0 + bias \n",
        "y_1 = weight * x_1 + bias\n",
        "\n",
        "# Plot our regression line from (x_0, y_0) to (x_1, y_1).\n",
        "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
        "\n",
        "# Label the graph axes.\n",
        "plt.ylabel(\"median_house_value\")\n",
        "plt.xlabel(\"total_rooms\")\n",
        "\n",
        "# Plot a scatter plot from our data sample.\n",
        "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
        "\n",
        "# Display graph.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0lRt4USU81L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This initial line looks way off.  See if you can look back at the summary stats and see the same information encoded there.\n",
        "\n",
        "Together, these initial sanity checks suggest we may be able to find a much better line."
      ]
    },
    {
      "metadata": {
        "id": "AZWF67uv0HTG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tweak the Model Hyperparameters\n",
        "For this exercise, we've put all the above code in a single function for convenience. You can call the function with different parameters to see the effect.\n",
        "\n",
        "In this function, we'll proceed in 10 evenly divided periods so that we can observe the model improvement at each period.\n",
        "\n",
        "For each period, we'll compute and graph training loss.  This may help you judge when a model is converged, or if it needs more iterations.\n",
        "\n",
        "We'll also plot the feature weight and bias term values learned by the model over time.  This is another way to see how things converge."
      ]
    },
    {
      "metadata": {
        "id": "wgSMeD5UU81N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n",
        "  \"\"\"Trains a linear regression model of one feature.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
        "      to use as input feature.\n",
        "  \"\"\"\n",
        "  \n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "\n",
        "  my_feature = input_feature\n",
        "  my_feature_data = california_housing_dataframe[[my_feature]]\n",
        "  my_label = \"median_house_value\"\n",
        "  targets = california_housing_dataframe[my_label]\n",
        "\n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
        "  \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
        "  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
        "  \n",
        "  # Create a linear regressor object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "      feature_columns=feature_columns,\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "\n",
        "  # Set up to plot the state of our model's line each period.\n",
        "  plt.figure(figsize=(15, 6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title(\"Learned Line by Period\")\n",
        "  plt.ylabel(my_label)\n",
        "  plt.xlabel(my_feature)\n",
        "  sample = california_housing_dataframe.sample(n=300)\n",
        "  plt.scatter(sample[my_feature], sample[my_label])\n",
        "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  root_mean_squared_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    # Take a break and compute predictions.\n",
        "    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
        "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
        "    \n",
        "    # Compute loss.\n",
        "    root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(predictions, targets))\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    root_mean_squared_errors.append(root_mean_squared_error)\n",
        "    # Finally, track the weights and biases over time.\n",
        "    # Apply some math to ensure that the data and line are plotted neatly.\n",
        "    y_extents = np.array([0, sample[my_label].max()])\n",
        "    \n",
        "    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
        "    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
        "\n",
        "    x_extents = (y_extents - bias) / weight\n",
        "    x_extents = np.maximum(np.minimum(x_extents,\n",
        "                                      sample[my_feature].max()),\n",
        "                           sample[my_feature].min())\n",
        "    y_extents = weight * x_extents + bias\n",
        "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.ylabel('RMSE')\n",
        "  plt.xlabel('Periods')\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(root_mean_squared_errors)\n",
        "\n",
        "  # Output a table with calibration data.\n",
        "  calibration_data = pd.DataFrame()\n",
        "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "  calibration_data[\"targets\"] = pd.Series(targets)\n",
        "  display.display(calibration_data.describe())\n",
        "\n",
        "  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kg8A4ArBU81Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Task 1:  Achieve an RMSE of 180 or Below\n",
        "\n",
        "Tweak the model hyperparameters to improve loss and better match the target distribution.\n",
        "If, after 5 minutes or so, you're having trouble beating a RMSE of 180, check the solution for a possible combination."
      ]
    },
    {
      "metadata": {
        "id": "UzoZUSdLIolF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    learning_rate=0.00001,\n",
        "    steps=100,\n",
        "    batch_size=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajVM7rkoYXeL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "metadata": {
        "id": "T3zmldDwYy5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    learning_rate=0.00002,\n",
        "    steps=500,\n",
        "    batch_size=5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8H0_D4vYa49",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is just one possible configuration; there may be other combinations of settings that also give good results. Note that in general, this exercise isn't about finding the *one best* setting, but to help build your intutions about how tweaking the model configuration affects prediction quality."
      ]
    },
    {
      "metadata": {
        "id": "zoD41nAmVUxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Is There a Standard Heuristic for Model Tuning?\n"
      ]
    },
    {
      "metadata": {
        "id": "QU5sLyYTqzqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a commonly asked question. The short answer is that the effects of different hyperparameters are data dependent. So there are no hard-and-fast rules; you'll need to test on your data.\n",
        "\n",
        "That said, here are a few rules of thumb that may help guide you:\n",
        "\n",
        " * Training error should steadily decrease, steeply at first, and should eventually plateau as training converges.\n",
        " * If the training has not converged, try running it for longer.\n",
        " * If the training error decreases too slowly, increasing the learning rate may help it decrease faster.\n",
        "   * But sometimes the exact opposite may happen if the learning rate is too high.\n",
        " * If the training error varies wildly, try decreasing the learning rate.\n",
        "   * Lower learning rate plus larger number of steps or larger batch size is often a good combination.\n",
        " * Very small batch sizes can also cause instability.  First try larger values like 100 or 1000, and decrease until you see degradation.\n",
        "\n",
        "Again, never go strictly by these rules of thumb, because the effects are data dependent.  Always experiment and verify."
      ]
    },
    {
      "metadata": {
        "id": "GpV-uF_cBCBU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Task 2: Try a Different Feature\n",
        "\n",
        "See if you can do any better by replacing the `total_rooms` feature with the `population` feature.\n",
        "\n",
        "Don't take more than 5 minutes on this portion."
      ]
    },
    {
      "metadata": {
        "id": "YMyOxzb0ZlAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ci1ISxxrZ7v0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "metadata": {
        "id": "SjdQQCduZ7BV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_model(\n",
        "    learning_rate=0.00002,\n",
        "    steps=1000,\n",
        "    batch_size=5,\n",
        "    input_feature=\"population\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DskmJEsu8xdX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Enable TPU Support in Colab"
      ]
    },
    {
      "metadata": {
        "id": "rrWQ83Cd81dN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![TPU](https://user-images.githubusercontent.com/58792/53053297-d14f2400-3455-11e9-836d-3857de1f4337.png)"
      ]
    },
    {
      "metadata": {
        "id": "QDDm6TsK34PC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras"
      ]
    },
    {
      "metadata": {
        "id": "ItXfxkxvosLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text classification with movie reviews (Project)"
      ]
    },
    {
      "metadata": {
        "id": "hKY4XMc9o8iB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Modified based on this tutorial: * https://research.google.com/seedbank/seed/classify_movie_reviews_using_tfkeras\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/basic_text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "Eg62Pmz3o83v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem. \n",
        "\n",
        "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
      ]
    },
    {
      "metadata": {
        "id": "2ew7HTbPpCJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZ8c25ug_FTf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Ingest"
      ]
    },
    {
      "metadata": {
        "id": "iAsKG535pHep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Download the IMDB dataset\n",
        "\n",
        "The IMDB dataset comes packaged with TensorFlow. It has already been preprocessed such that the reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary.\n",
        "\n",
        "The following code downloads the IMDB dataset to your machine (or uses a cached copy if you've already downloaded it):"
      ]
    },
    {
      "metadata": {
        "id": "zXXx5Oc3pOmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imdb = keras.datasets.imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odr-KlzO-lkL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The argument `num_words=10000` keeps the top 10,000 most frequently occurring words in the training data. The rare words are discarded to keep the size of the data manageable."
      ]
    },
    {
      "metadata": {
        "id": "l50X3GfjpU4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Explore the data (EDA)\n",
        "\n",
        "Let's take a moment to understand the format of the data. The dataset comes preprocessed: each example is an array of integers representing the words of the movie review. Each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review."
      ]
    },
    {
      "metadata": {
        "id": "y8qCnve_-lkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnKvHWW4-lkW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The text of reviews have been converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:"
      ]
    },
    {
      "metadata": {
        "id": "QtTS4kpEpjbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIE4l_72x7DP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Movie reviews may be different lengths. The below code shows the number of words in the first and second reviews. Since inputs to a neural network must be the same length, we'll need to resolve this later."
      ]
    },
    {
      "metadata": {
        "id": "X-6Ii9Pfx6Nr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wJg2FiYpuoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Convert the integers back to words\n",
        "\n",
        "It may be useful to know how to convert integers back to text. Here, we'll create a helper function to query a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "metadata": {
        "id": "tr5s_1alpzop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3CNRvEZVppl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can use the `decode_review` function to display the text for the first review:"
      ]
    },
    {
      "metadata": {
        "id": "s_OqxmH6-lkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decode_review(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFP_XKVRp4_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Prepare the data\n",
        "\n",
        "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network. This conversion can be done a couple of ways:\n",
        "\n",
        "* Convert the arrays into vectors of 0s and 1s indicating word occurrence, similar to a one-hot encoding. For example, the sequence  [3, 5] would become a 10,000-dimensional vector that is all zeros except for indices 3 and 5, which are ones. Then, make this the first layer in our network—a Dense layer—that can handle floating point vector data. This approach is memory intensive, though, requiring a `num_words * num_reviews` size matrix.\n",
        "\n",
        "* Alternatively, we can pad the arrays so they all have the same length, then create an integer tensor of shape `max_length * num_reviews`. We can use an embedding layer capable of handling this shape as the first layer in our network.\n",
        "\n",
        "In this tutorial, we will use the second approach. \n",
        "\n",
        "Since the movie reviews must be the same length, we will use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) function to standardize the lengths:"
      ]
    },
    {
      "metadata": {
        "id": "2jQv-omsHurp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VO5MBpyQdipD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the length of the examples now:"
      ]
    },
    {
      "metadata": {
        "id": "USSSBnkE-lky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJoxZGyfjT5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And inspect the (now padded) first review:"
      ]
    },
    {
      "metadata": {
        "id": "TG8X9cqi-lk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLC02j2g-llC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Modeling\n",
        "\n",
        "The neural network is created by stacking layers—this requires two main architectural decisions:\n",
        "\n",
        "* How many layers to use in the model?\n",
        "* How many *hidden units* to use for each layer?\n",
        "\n",
        "In this example, the input data consists of an array of word-indices. The labels to predict are either 0 or 1. Let's build a model for this problem:"
      ]
    },
    {
      "metadata": {
        "id": "xpKOoWgu-llD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
        "vocab_size = 10000\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PbKQ6mucuKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "1. The first layer is an `Embedding` layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
        "2. Next, a `GlobalAveragePooling1D` layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "3. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
        "4. The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confidence level."
      ]
    },
    {
      "metadata": {
        "id": "Bn9ukmAb_pKE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Hidden units"
      ]
    },
    {
      "metadata": {
        "id": "0XMwnDOp-llH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The above model has two intermediate or \"hidden\" layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
        "\n",
        "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*, and we'll explore it later."
      ]
    },
    {
      "metadata": {
        "id": "L4EqVWg4-llM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Loss function and optimizer\n",
        "\n",
        "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
        "\n",
        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
        "\n",
        "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "metadata": {
        "id": "Mr0GP-cQ-llN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCWYwkug-llQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Create a validation set\n",
        "\n",
        "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a *validation set* by setting apart 10,000 examples from the original training data. (Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy)."
      ]
    },
    {
      "metadata": {
        "id": "-NpcXY9--llS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "35jv_fzP-llU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Train the model\n",
        "\n",
        "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
      ]
    },
    {
      "metadata": {
        "id": "tXSGrjWZ-llW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EEGuDVuzb5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "zOMKywn4zReN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1iEXVTR0Z2t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
      ]
    },
    {
      "metadata": {
        "id": "5KggXVeL-llZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Create a graph of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "metadata": {
        "id": "VcvSXvhp-llb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRKsqL40-lle",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "nGoYf2Js-lle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hXx-xOv-llh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFEmZ5zq-llk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy—they seem to peak after about twenty epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
        "\n",
        "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs. Later, you'll see how to do this automatically with a callback."
      ]
    },
    {
      "metadata": {
        "id": "NvoiEwiAWrWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ludwig (Open Source AutoML)"
      ]
    },
    {
      "metadata": {
        "id": "jbnbFKcTXNOn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Github Project URL**:  https://uber.github.io/ludwig/\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/58792/52920000-f78d8c00-32bc-11e9-8e5e-adf53f1b8a37.png)"
      ]
    },
    {
      "metadata": {
        "id": "Aa-KnZKcfvkV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install Ludwig"
      ]
    },
    {
      "metadata": {
        "id": "Q3FrtesdfyV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy #must restart colab runtime\n",
        "!pip install --upgrade scikit-image\n",
        "!pip install -q ludwig\n",
        "!python -m spacy download en "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVLSSAUViyiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Ideas"
      ]
    },
    {
      "metadata": {
        "id": "qHaRqAN5i1iV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **Training Models**\n",
        "* **Prediction (Inference)**\n",
        "* **Datatypes**\n",
        " - binary\n",
        " - numerical\n",
        " - category\n",
        " - set\n",
        " - bag\n",
        " - sequence\n",
        " - text\n",
        " - timeseries\n",
        " - image\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "W3G5sZ3yo-yK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification of Book Title Categories (Project)"
      ]
    },
    {
      "metadata": {
        "id": "aIbXYrxU8ySd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/uchidalab/book-dataset/master/Task1/book30-listing-train.csv\n",
        "!wget https://raw.githubusercontent.com/noahgift/recommendations/master/model_definition.yaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-w5Zkzcumoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Ingest"
      ]
    },
    {
      "metadata": {
        "id": "Ef8dbaV4tHrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://media.githubusercontent.com/media/noahgift/recommendations/master/data/book30-listing-train-with-headers.csv\")\n",
        "df = df.drop(\"Unnamed: 0\", axis=1)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efw0icqJ_0Bq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv(\"book30-listing-train-with-headers.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wR_L2OPkuqH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### EDA"
      ]
    },
    {
      "metadata": {
        "id": "IUGz5U5duj-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Columns**"
      ]
    },
    {
      "metadata": {
        "id": "KVYJIiwHuhiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ir0Viy2zuyr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Shape**"
      ]
    },
    {
      "metadata": {
        "id": "-kaJsKyruyAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJNSA3n0u3Zf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training w/Ludwig"
      ]
    },
    {
      "metadata": {
        "id": "bldBWuL2Nwmh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head book30-listing-train-with-headers.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ous6EqC8Nocg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat model_definition.yaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpVA2fyXLRoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ludwig experiment --data_csv book30-listing-train-with-headers.csv --model_definition_file model_definition.yaml\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21q0EGuO5dpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0ExRA9v55cr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggjuE4es7SDH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GCP AutoML\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ydR9SXDKAXvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![GCP AutoML](https://user-images.githubusercontent.com/58792/45260264-134c4800-b397-11e8-9832-fd56a8eeaa3c.png)\n",
        "\n",
        "\n",
        "**[GCP AutoML Products](https://cloud.google.com/automl/)**\n",
        "\n",
        "*  [AutoML Vision](https://cloud.google.com/vision/automl/docs/)\n",
        "*  [AutoML Natural Language](https://cloud.google.com/natural-language/automl/docs/)\n",
        "*  [AutoML Translation](https://cloud.google.com/translate/automl/docs/)\n"
      ]
    },
    {
      "metadata": {
        "id": "QYU7DysIifKH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GCP AutoML Demo\n"
      ]
    },
    {
      "metadata": {
        "id": "MAtZVZgRin7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training to recognize flowers\n",
        "\n",
        "![upload_data](https://user-images.githubusercontent.com/58792/45438280-94dbf880-b66b-11e8-850d-e7d2a32c45c8.png)\n",
        "\n",
        "![train](https://user-images.githubusercontent.com/58792/45438281-94dbf880-b66b-11e8-9951-80202c581b0c.png)\n",
        "\n",
        "![predict](https://user-images.githubusercontent.com/58792/45439236-1fbdf280-b66e-11e8-9f51-d92d26a63c64.png)"
      ]
    },
    {
      "metadata": {
        "id": "tho4WI7ikZb0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GCP Vision API"
      ]
    },
    {
      "metadata": {
        "id": "desIuhDTkogY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vision/\n",
        "\n",
        "![Vision API](https://cloud.google.com/images/products/vision/image-search.svg)\n",
        "\n",
        "* [Step 1:  Enable API](https://cloud.google.com/vision/docs/before-you-begin)"
      ]
    },
    {
      "metadata": {
        "id": "emW2WhQYn5e7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "hPWO_zyRopXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Google Credentials with GDrive\n"
      ]
    },
    {
      "metadata": {
        "id": "XI73HZNLobp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "deab35c9-8305-4e5a-d2e7-1ec5550162db"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UNyzZwgmoxwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80fb3d9e-a93f-42ee-a4c7-cf5c2385b367"
      },
      "cell_type": "code",
      "source": [
        "import os;os.listdir(\"/content/gdrive/My Drive/awsml\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kaggle.json', 'credentials', 'config', 'cloudai-7ab42a4d8a43.json']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "k7LLMTLNuxgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a07744b3-8114-4869-c2b5-d72da86856c1"
      },
      "cell_type": "code",
      "source": [
        "!ls -la /content/gdrive/My\\ Drive/awsml"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 2305 Feb 22 23:47 cloudai-7ab42a4d8a43.json\n",
            "-rw------- 1 root root   43 Nov 22 00:05 config\n",
            "-rw------- 1 root root  117 Nov 22 00:01 credentials\n",
            "-rw------- 1 root root   64 Nov 21 22:24 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w2f1xHUMwCJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install cloud vision Api"
      ]
    },
    {
      "metadata": {
        "id": "sRlg-4kPk1Wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d17fdcd7-5bd8-4425-a0f0-9c61543ac1a2"
      },
      "cell_type": "code",
      "source": [
        "!export GOOGLE_APPLICATION_CREDENTIALS=\"/content/gdrive/My\\ Drive/awsml/cloudai-7ab42a4d8a43.json\"\n",
        "!gcloud auth activate-service-account --key-file /content/gdrive/My\\ Drive/awsml/cloudai-7ab42a4d8a43.json\n",
        "!pip install --upgrade -q google-cloud-vision"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activated service account credentials for: [pragai@cloudai-194723.iam.gserviceaccount.com]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oIJgeisov5kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d84170e2-33f9-472c-9bad-28134410ddc2"
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/gdrive/My Drive/awsml/cloudai-7ab42a4d8a43.json\"\n",
        "\n",
        "# Imports the Google Cloud client library\n",
        "from google.cloud import vision\n",
        "from google.cloud.vision import types\n",
        "\n",
        "# Instantiates a client\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# The name of the image file to annotate\n",
        "file_name = 'wakeupcat.jpg'\n",
        "\n",
        "# Loads the image into memory\n",
        "with io.open(file_name, 'rb') as image_file:\n",
        "    content = image_file.read()\n",
        "\n",
        "image = types.Image(content=content)\n",
        "\n",
        "# Performs label detection on the image file\n",
        "response = client.label_detection(image=image)\n",
        "labels = response.label_annotations\n",
        "\n",
        "print('Labels:')\n",
        "for label in labels:\n",
        "    print(label.description)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels:\n",
            "Cat\n",
            "Mammal\n",
            "Small to medium-sized cats\n",
            "Whiskers\n",
            "Felidae\n",
            "Nose\n",
            "Carnivore\n",
            "Eye\n",
            "Snout\n",
            "Fur\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tBbAdZspTRGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n"
      ]
    },
    {
      "metadata": {
        "id": "PjOc_DzTTU8v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AiFKjr0FTaDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}